This project demonstrates a multi-task learning setup using a transformer-based model to handle multiple NLP tasks (e.g., sentence classification, named entity recognition).
The goal is to show how to build a shared transformer encoder with separate task-specific heads, manage different training strategies (freezing/unfreezing layers), and implement a basic multi-task training loop.

Environment Setup:
Create or activate your Python virtual environment.
Install the dependencies:
pip install -r requirements.txt
This ensures all the necessary libraries (e.g., PyTorch, Hugging Face Transformers) are installed and configured before running the code. Once installed, you can run the example scripts and notebooks to see the model in action.